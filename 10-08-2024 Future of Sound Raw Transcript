:
I really sometimes think that accessibility kind of deserves its own category, but it is really, you know, in the context of people with disabilities, people with even sometimes different learning styles, different ways of engaging, like, are you giving those people an easy pathway to still access and do something with your brand, right? And sometimes artificial intelligence, I think, can help in those areas, but other times it can hurt. And we haven't fully tested that. And so conducting user testing, like really, really thoughtful testing with a diverse group of people, ideally people also have disabilities, I think that's really important. Welcome to the Future in Sound podcast. I'm your host, Jen Wilson. This is a podcast where we discuss people, planet, and profit. In each episode, we'll learn from world-leading experts who can help us see the future we want and our role in it. And this is episode 33, trust in AI. Jackson Kahn is a former senior policy advisor to the Canadian Minister of Innovation, Science, and Industry. Prior to government, Jackson worked in technology, including as director of growth at Fable, an accessibility platform that helps major organizations build more inclusive digital products for people with disabilities. He's also worked for companies like Nudge.ai, which is acquired by Affinity, Influitive, Paddle, which was acquired by LinkedIn, and Microsoft. Jackson is a published author and speaker on technology education and policy, including with the International Economic Development Council and also TEDx. All right, Jackson, it is great to be here with you today. a topic that has been front of mind to many who will be The implications of AI on their businesses, what the role of business is in managing risks related to AI. I can't think of anybody who'd be better placed. to help us think through some of the sort of connections between technology and society. So welcome to the Future In Sound podcast. Thanks so much, Jen. It's really great to be here with you. Thanks for the very kind words. Always happy to chat about the topic. I think I've just come off of a very interesting time working on the problem in federal politics in Canada, so happy to dive into it with you. Sounds great. And why don't we start for listeners who aren't familiar with your work. Why don't we start with just a 60 to 90 second intro on who you are and the work that you've done. Yeah, sounds good. I can keep it short, but basically I was a senior policy advisor to Canada's Minister of Innovation, Science, and Industry. One of the key files that we were working on the last couple of years was definitely related to AI safety, building AI trust, and also accelerating AI innovation. So one of the big things that we thought about is, well, how do we actually create a system of laws that can be flexible to grow at the speed of innovation? but also make sure that Canadians and consumers at large are protected in our society. And we're also very thoughtful to the concerns of creators, artists, intellectual property makers, whatever it is, we wanna make sure that AI is developed safely and responsibly. And Canada historically is huge place in AI, right? We have invented some of the key technologies like natural language processing, machine learning ethics is something we've worked on for a long time. So I feel very, very happy to, and privileged to have come from that spot. And then before I worked in government, I actually worked in tech startups for a number of years. including early AI startups and I run my own AI podcast. It's a little bit of a meta moment where a podcaster is interviewing a podcaster. I mean, obviously you've built businesses that are using AI Why don't we start? of those who are experts in AI are looking at policy, let's move fast and break things, because there's so much opportunity. at one end of the scale. we need to proceed with caution. And there are a variety of challenges that are ways that this technology could be used to undermine democracies or to have sort of negative consequences. Where are you on that sort of scale? Yeah, I try not to plot myself too specifically, not just to remain mysterious, but actually just keep an open mind. I think as a policymaker in the last couple of years, it has been moving so quickly. And I've had a chance to sit down with some of the most esteemed. thought leaders in the world, everyone from some of the most foundational AI researchers, in some cases actually the godfathers of AI. I've also talked to some of the top chipmakers and leaders of those firms. And on the other side of the spectrum, I also talked to business leaders who, you know, some are very concerned about what AI can do, they're concerned about what might it do to their industries. Others are so bullish on artificial intelligence that I think any regulations in the way of that, any frameworks are actually the worst thing that we could do for society. And so in the midst of all that, what I try and do is think about... Well, what is the average person? What do they care about? They probably care about their kids. They probably care about their job and the kind of life that they're going to have. Technological change can always be scary. It can also be very exciting. We know there's an innovation adoption curve. Some people are more comfortable with it. Others aren't. I think with AI, we have a more profound and potential technology than ever. It's so high potential. Some people have compared the emergence of AI to the emergence of the internet. I actually heard a more apt analogy, which I thought was more like the invention of electricity in that it is just so... It's going to permeate every kind of area of society and work and learning. We're still trying to really understand what it is. That's why I'm a little hesitant to say yes, I'm an AI accelerationist or decelerationist. I generally think, I grew up with a dad who's a software architect, so I've always been a huge geek. I love new tech. At the same time, I grew up with a mother who has worked in the education system with some of the most vulnerable. I draw a lot from my parents. I draw a lot from my ancestors. I draw a lot from growing up in a multicultural society. I think we've got to do the best we can to end up in a good place that doesn't leave folks behind. but that also starts to paint a picture where we can end up with a better world. So, you know, I don't want that to sound like platitudes, but the bottom line is I, um... I really think that we have something great here, and I'm also very conscious of the national security element where countries around the world, some of who we are in alliance with and others who we are in sometimes rival or competition with, are trying to develop this as fast as they can. In the midst of all that, can we set a responsible course? Can we set one that's safe? How do we build trust over time? That's really the questions that have been occupying my mind. How do we do those things? What are the most important tenets of the policy direction before we get into the business level? What's really important for us? to consider if war to develop AI for good and the great outcomes that meet its potential. Yeah, I mean, if I was to respond in a quick and dirty way to that question, the first thing is you've got to talk to people who are building the stuff. You really got to understand from the basic technical dimensions, how is this working? How fast can it grow? How are you training the models? Not how big can they get, but basically how complex can they get? I still think we're trying to answer those questions, but I think talking to technologists is very important. At the same time, I think sometimes we can have a fallacy where we sometimes paint technologists public policy. That's not always the case. They can have great insights, but just trying to ascribe policy exactly to the parameters of the people who are also designing it. If we did that in every case, I think we could be in trouble. There's also instances where some of the most prominent investors in tech, I believe the name is Bill Gurley, who gave this kind of famous talk more recently. It was about why is innovation done in Silicon Valley? It was because it's far away from Washington. I would poke some holes in that. I also think that Washington is also responsible for some of foundational R&D investments, right, including in the internet, including some of the most, you know, early stages of defense research, right? And so there are the most the most dramatic advances in society, I think, have often come through a fusion of government sponsored efforts that then meet. Innovation and ideally then government can get out of the way to some extent But then if it gets very powerful like you do that have to ask okay Well, how do we make sure that the technology is adapted responsibly? So I think you got to talk to the technologists, but then you've got to go and make sure throughout the whole process You're talking to citizens you're talking to civil society groups who you know see different faucets of how does this actually end up? You have to talk to academics and then also what I tried to do was I tried to talk to the people who are most critical and sometimes that meant meeting with critics more than you know people who are just slapping us on the back and say, yep, let's get it going and let's pour all of our money into this. And I think critically evaluating criticism, I guess to be very flunky with my words, I think was very important because it really made us think, it made us challenge some of the assumptions we may have held. A lot of it was also even educating people internally within the public service. Some of the public servants that I worked with didn't necessarily come from a technical background. They didn't have a great understanding of what is it actually like to work in a startup. So part of that innovation life cycle, it's important to educate people on how quickly can this actually come about and make sure you're facilitating those connections between policymakers and the innovation community. With that context of... ensuring we're talking to critics, ensuring we're keeping sort of an open mind, there's solid discourse to help us steer in the right direction. What kinds of policies do you see in the next, I know it's very difficult to answer this question, but kind of policies, not specifically in Canada, let's say, you know, globally, what kinds of policies do you think we're likely to see in the coming, say, five to ten years related to AI, if any? Yeah, I think there's kind of two big examples that we have right now, at least in the Western world. One is kind of the US approach where they have the White House executive order on AI. So some examples there would be there's pieces around procuring artificial intelligence, orders to federal agencies on how to handle that. There's also the White House agreement, the Biden commitments on artificial intelligence, which were actually voluntary commitments from some of the top AI companies, including Anthropic, OpenAI. And those actually stipulating certain types of security requirements and actually then I think these are all great things, right? They're voluntary, they can move very quickly, they can move at the speed of industry, but particularly with the force of the US government behind them, I think they carry a lot of weight. And also these companies, they have a prerogative to try and build public trust and understanding. I like that that's very flexible. Some advocates in civil society and citizens might say, well, yeah, but there don't seem to be any clear consequences. What are the penalties for doing wrong? And then further on, folks who are very worried about AI safety, it's like, well, how are we actually trying to prevent catastrophic risk? It's just safety testing enough. Like we really need to, you know, maybe put the fear of God in some of these people because they are creating something that could be very powerful. The other side of that, of course, is like, you know, the EU AI Act, which, you know, very prescriptive, has a lot of, like an extensive amount of detail, unacceptable categories of risk, all sorts of different parameters there. I'm fascinated by both models. I think in Canada, we tried to, it might take a little bit of the best of both. In the interim, we had created our own volunteer code of conduct. We tried to model some of the best practices from what we saw from the US perspective, applications for business to business applications, for example, in the enterprise, are actually quite different than the ones that are consumer facing. There's different levels of risk, different levels of application. And so we tried to differentiate there. And I would argue that I think we've done something pretty cool there. And we got, you know, all the top AI companies in Canada and the top technology companies signed up to it. On the other side, we had an AI law that we've been working on and pushing through our house of commons in parliament for the last couple of years. It is still an industry committee study, but that tried to set some high level parameters and categories of risk, but then have a lot of the flexibility left to regulations. It remains to be seen if that bill will pass, particularly given that the Canadian government is a minority situation. But even if the bill doesn't pass, I think it's very useful that this has been a tool for the government, which I've recently left, by the way, to collect feedback, to understand from civil society what's gone right and what's gone wrong, from industry, the same thing. And I'm hopeful that people will continue to experiment and try different models. And even if you get negative feedback, I think we're trying to figure this out, right? This really just became, I feel like, more of a theoretical area into something that governments around the world are chasing. and trying to figure out how do we appropriately regulate artificial intelligence. The last thing I would say is, you know, countries like Singapore, for example, are doing a lot of innovative work and trying to figure out how could this baby be done better, and I think it's worth looking at. China's even ahead, you know, in some respects on trying to regulate artificial intelligence. So I ultimately think there will also have to be some sort of international agreements on what comes out. You know, there's the Council of Europe work that's potentially being done on this. The OECD is playing a significant role. I also think organizations like Mozilla, for example, which are quite prominent on developing, definitions on artificial intelligence, I think they're playing a strong role in actually advancing new and interesting concepts of how we can define what we're actually working with here. Hey, it's Jen. I just wanted to take a quick moment to let you know a bit about Ricoh and what we do. We're a tech-enabled advisory firm that helps private market investors and companies measure sustainability metrics using our software platform. We also help you to set targets and focus your efforts on sustainability areas that really matter for your business. And finally, we help clients to translate all of this work into your core value creation strategy or your business model. Check us out at re.co.com to get in touch. All right, now back to our conversation. It's really interesting because it's so competitive and there's so much opportunity for competitive advantage with AI that often when we look at more ESG metrics, there's this question of, okay, is there going to be a specific standard that comes to the fore like GDPR, which then for international companies becomes the standard because you're operating globally therefore, adhering to GDPR makes sense. But with... you know, this scenario, it sounds like it's going to be a bit more bespoke. I'm interested in pivoting a little bit to the business perspective. So for example, Rico, you know, we are developing tools all the time with AI. And of course we're advising clients on business ethics, future of capitalism, the right metrics to have. And many of our clients are technology companies. So what are some of the tips that you would have for business executives who are running technology companies and managing the risks related to AI? while pursuing the opportunities. Yeah, I mean, I think you gotta start with... what it is that you're actually doing right now, what it is that you want to do, and really understand the possible implications of that. That might sound pretty simplistic, but I think sometimes it can be easy to get ahead of yourself when you're bored, your shareholders are pushing for you to take action and do an AI thing sooner rather than later. So I think it's really taking a pause and being thoughtful about this. I also think it's worth really engaging with your users and consumers on this front. I think right now, because artificial intelligence in general is under high scrutiny, I feel like a ton of screw ups. From even some major big brands who've used artificial intelligence to generate images without credit even for example, and experiencing pushback from artists or even communities of people on social media about that. So something that was meant to be good is immediately turned back and around. We're seeing political campaigns do this and having it backfire sometimes. So I think it's really, really important to operate with a much higher level of responsibility, accountability, and respect. There are global principles of transparency. that are often talked about in ethics communities. But I think before you even think about artificial intelligence, ethics, and how we're actually operating this, I think it's doing some common sense gut checks sometimes and really just thinking through, if someone took the most cynical view of this possible in our intended application, what is it going to look like? How can the media talk about it? How could our customers talk about it? I don't know if this is the segue you want to take, John, but I do think a lot of it comes down to trust. And Edelman, their trust barometer is like an annual metric-based survey they kind of share every year. And one of the special reports, I believe, had been focused on artificial intelligence and found that trust levels, I believe, are kind of below 50%, I think, in most major Western economies. I think Canada was one of the lower ones. US even, I think, was not doing very well. I think Pew Research survey is saying most Americans, you know, unless there was some the use of AI would not necessarily trust it very much. The rules would certainly help. And so I think we've got to overcome a trust gap that is prevalent, that does exist. And I think as citizens, as technologists, policymakers, we've all got a job to do, and business owners to try and build trust in anything that we want to do before it comes to our customers. Really practically, if you're on the board or if you're the CEO of a technology company, That's not, you're not building large language models. You're using models in your business. or on an annual basis to ensure that you're keeping your technology in check and that you are managing some of the risks while also pursuing opportunities? Yeah, I'd have to think about that for a second, but I think foundation, it's like at what speed are we delivering this, to what extent do we know what types of results the model... Like producing, like if you're using a pre-existing model or, you know, one of the consumer based ones, you may not always be able to control the outcomes. These models are updating very frequently. The data that they're capturing can change. Sometimes they're not getting the latest information, right? Um, and necessarily from online, depending on what types of things you're using, whatever API, like running out of credits on your API, like you want to make sure that you have some control over the types of variables, some ways you might be able to control that better is certainly fine tuning your model, using your own private data. Those are the questions I would try and ask is, do we know what kind of outcomes we're going to produce? Do we know where the data is coming from? Is this found in actually our terms of service? Is it updated? What kind of data are we gathering from the people that we are asking to use this tool, if any? And then, you know, if you wanted to go even further, you could start to think about the safety implications. But I don't know, I usually think more about the practical ones right now, less kind of the catastrophic or existential ones or where those could go. I think most people are not even close to there yet. But I think a lot more about, you know, are artists going to be pissed off depending on how someone's using this? trying to create new content. Are people in my own company, my own employees going to be pissed off if this model that goes and spits out something that is incorrigible or just doesn't make sense with our own processes? In some cases, AI chatbots have given people refunds and consumer tech companies have had to go and figure that out, right? And they've got pissed off customers because they thought they were talking to someone real but that wasn't the case. So you really just got to check it. Check it rather than having a black box. So I think that's a fair, reasonable expectation, right? Again, like I am not. I'm not an AI engineer, I'm not a research scientist. I don't even know to the deep technical extent how some of these work. I think what I've tried to do, having some background in tech and growth and in policy, is understand what does this typically mean for the average person? How can we be more honest about what this actually means and not just get excited about what this is going to look like on a spreadsheet in my next quarter's numbers. I think a lot of people are getting very excited. Policy makers and economists are excited about the productivity aspects. I think businesses are excited about new channels of growth. That's awesome. things for our economy, our society. But the average person is still like, well, I just got to do a thing and get it done. I got to buy a product. I got to get a service done. And unless this actually makes my life better, it could just drive more frustration. Sometimes people could encounter an AI chatbot and be like, well, I just need to talk to a human right now about a thing. And so making sure there's still an ease of access path that is there. The last thing I would say quickly is more and more brands, and this is somewhat connected to ESG. And to some extent, DEI, I suppose, but I really sometimes think that accessibility kind of deserves its own category, but it is really, you know, in the context of people with disabilities, people with even sometimes different learning styles, different ways of engaging, like, are you giving those people an easy pathway to still access and do something with your brand? Right. And sometimes artificial intelligence, I think, can help in those areas, but other times it can hurt. And we haven't fully tested that. And so conducting user testing, like really, really thoughtful testing with a diverse group of people, ideally people also have disabilities. I think that's really important. example of AI not being great for somebody with disabilities just so the audience can understand. I will give a quick estimate of guess, but my strong advice is to ask someone with a disability, which I am not. I think there's great companies out there who can provide insight there. I actually used to work for one called Fable, so feel free to talk to them. An example I can think of off the top of my head is a lot of people who are blind or who have visual impairments often use a screen reader to interact with a device. If for example the focus of your device, let's say a computer or a phone, just the focus, That can be confusing, particularly if there's no actual audio cue to the person who is using a screen reader, which delivers voice messages to someone who again may have a visual impairment. That can be confusing, right? So it's like, are you as a product designer, as an engineer or builder, are you actually making this an easier experience, right? At the same time, AI can also have awesome benefits, right? There could be translation, there could be better audio, right? There could be more thoughtful engagement of someone. Maybe there can be even a recognition that, yeah, oh yeah, this is clearly someone with browsing using my website, I think based on the characteristics, maybe we should put up a prompt and inquire, or we should start to serve at a different user interface. There are a lot of opportunities, but I think the most important thing is don't make assumptions, especially if you're able-bodied. If you don't have experience, a little bit of experience in that area, it's really, really important to think thoughtfully about it because people with disabilities are like 15% of the global population. I think it's a billion people, and it's one in two seniors. Thank you for that. My final question, Jackson, for you is... Many people listening to this will be thinking, I just want to keep up to date with some of the developments that matter. What resources should I be taking a look at? I'm wondering, are there any newsletters, books, anything that you read on a regular basis that helps you think more strategically about AI and technology that you'd be interested in sharing with the audience? Yeah, definitely. I mean, a lot of the stuff I read is like, really, really geeky. If I can think of something that I think is more useful for, for let's say a business person or just someone who's keen on understanding. bit better about how machine learning works and also generative in artificial intelligence. I think Goldfarb and Ajay Agarwal, and I think I'm forgetting one third writer, I apologize, but they wrote a book a few years back called Prediction Machines and they just wrote another book which I think I've ordered and I still got to read through, but they do a pretty darn good job of just explaining, you know, here's how this works, right? And I think that kind of accessible, useful content is really, really valuable. I also think, you know... My advice would actually be to stay away from stuff that is too simplistic. I think there's a lot of people on Twitter saying, you know, here's the seven ways that ChatGPT is going to change your life. I think it's died down a little bit since the earliest hype cycle, but you know, maybe try and go, you know, and look for experts who are still human and can, you know, talk to you in a real way. And that's kind of my advice is, you know, try and read books about this stuff. Don't just, you know, read the latest tweets. I think it's really, really important to get reasonably deep into it before you decide to go and change your whole business model or, you know, the way that you run your organization. Jackson, thank you so much. Right on, no worries, Jen. This is a lot of fun. The Future in Sound podcast is written and hosted by Jen Wilson and produced by Chris Attaway. This podcast is brought to you by Ricoh, a tech-powered advisory company helping private market investors pursue sustainability objectives and value creation in tandem. If you enjoyed this podcast, don't forget to tell a friend about it. And if you have a moment to rate us in your podcast app, we'd really appreciate it. Until next time, thanks for listening.